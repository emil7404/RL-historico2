{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4655afed",
   "metadata": {},
   "source": [
    "# DeepScalper: Dueling DQN con *Action Branching* (skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ToyIntradayEnv:\n",
    "    def __init__(self, T=2000, k_price=11, k_size=5):\n",
    "        self.T = T\n",
    "        self.t = 0\n",
    "        self.k_price = k_price\n",
    "        self.k_size = k_size\n",
    "        rng = np.random.default_rng(42)\n",
    "        self.p = 100 + np.cumsum(rng.normal(0, 0.02, size=T))\n",
    "        self.position = 0.0\n",
    "        self.cash = 0.0\n",
    "        self.inv = 0.0\n",
    "    def reset(self):\n",
    "        self.t=0; self.position=0.0; self.cash=0.0; self.inv=0.0\n",
    "        return self._obs()\n",
    "    def _obs(self):\n",
    "        lo = max(0, self.t-20)\n",
    "        win = self.p[lo:self.t+1]\n",
    "        pad = 21-len(win)\n",
    "        x = np.pad((win - win.mean())/(win.std()+1e-6), (pad,0))\n",
    "        return x.astype(np.float32)\n",
    "    def step(self, a_price_idx, a_size_idx):\n",
    "        done = False\n",
    "        price_levels = np.linspace(-0.1, 0.1, self.k_price)\n",
    "        size_levels = np.linspace(0.0, 1.0, self.k_size)\n",
    "        rel = price_levels[a_price_idx]\n",
    "        sz = size_levels[a_size_idx]\n",
    "        mid = self.p[self.t]\n",
    "        limit_price = mid*(1+rel)\n",
    "        executed = sz\n",
    "        self.cash -= executed*limit_price\n",
    "        self.inv += executed\n",
    "        future = self.p[min(self.t+1, self.T-1)]\n",
    "        pnl = self.inv*(future - mid)\n",
    "        bonus = -abs(limit_price - future)\n",
    "        reward = pnl + 0.01*bonus\n",
    "        self.t += 1\n",
    "        if self.t >= self.T-1:\n",
    "            done = True\n",
    "            self.cash += self.inv*self.p[self.t]\n",
    "            self.inv = 0.0\n",
    "            reward += self.cash*1e-4\n",
    "        return self._obs(), float(reward), done, {}\n",
    "\n",
    "class DuelingBranchQ(nn.Module):\n",
    "    def __init__(self, obs_dim, n_price, n_size):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "        )\n",
    "        self.value = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.adv_price = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, n_price))\n",
    "        self.adv_size  = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, n_size))\n",
    "    def forward(self, x):\n",
    "        h = self.feat(x)\n",
    "        V = self.value(h)\n",
    "        Ap = self.adv_price(h)\n",
    "        As = self.adv_size(h)\n",
    "        Q = V + (Ap - Ap.mean(dim=1, keepdim=True)) + (As - As.mean(dim=1, keepdim=True))\n",
    "        return Q, Ap, As\n",
    "\n",
    "def train_agent(episodes=10):\n",
    "    env = ToyIntradayEnv()\n",
    "    obs_dim = 21\n",
    "    n_price, n_size = env.k_price, env.k_size\n",
    "    net = DuelingBranchQ(obs_dim, n_price, n_size).to(device)\n",
    "    tgt = DuelingBranchQ(obs_dim, n_price, n_size).to(device)\n",
    "    tgt.load_state_dict(net.state_dict())\n",
    "    opt = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    buf = deque(maxlen=50000)\n",
    "    gamma = 0.99\n",
    "    eps = 1.0\n",
    "    batch = 128\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < eps:\n",
    "                a_p = random.randrange(n_price)\n",
    "                a_s = random.randrange(n_size)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    qs, Ap, As = net(torch.tensor(s).unsqueeze(0).to(device))\n",
    "                    a_p = Ap.argmax(dim=1).item()\n",
    "                    a_s = As.argmax(dim=1).item()\n",
    "            s2, r, done, _ = env.step(a_p, a_s)\n",
    "            buf.append((s, a_p, a_s, r, s2, done))\n",
    "            s = s2\n",
    "            if len(buf) >= batch:\n",
    "                batch_s, bp, bs, br, bs2, bd = zip(*random.sample(buf, batch))\n",
    "                bS = torch.tensor(np.stack(batch_s)).to(device)\n",
    "                bS2= torch.tensor(np.stack(bs2)).to(device)\n",
    "                bP = torch.tensor(bp).long().to(device)\n",
    "                bSiz= torch.tensor(bs).long().to(device)\n",
    "                bR = torch.tensor(br).float().to(device)\n",
    "                bD = torch.tensor(bd).float().to(device)\n",
    "                Q, Ap, As = net(bS)\n",
    "                Qsa = net.value(net.feat(bS)).squeeze() + Ap.gather(1, bP.unsqueeze(1)).squeeze() - Ap.mean(dim=1)                          + As.gather(1, bSiz.unsqueeze(1)).squeeze() - As.mean(dim=1)\n",
    "                with torch.no_grad():\n",
    "                    Qt, Apt, Ast = tgt(bS2)\n",
    "                    ap2 = Apt.argmax(dim=1)\n",
    "                    as2 = Ast.argmax(dim=1)\n",
    "                    Qt_next = tgt.value(tgt.feat(bS2)).squeeze() + Apt.gather(1, ap2.unsqueeze(1)).squeeze() - Apt.mean(dim=1)                                 + Ast.gather(1, as2.unsqueeze(1)).squeeze() - Ast.mean(dim=1)\n",
    "                    y = bR + (1.0 - bD)*gamma*Qt_next\n",
    "                loss = ((Qsa - y.detach())**2).mean()\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "        if ep % 2 == 1:\n",
    "            tgt.load_state_dict(net.state_dict())\n",
    "        eps = max(0.05, eps*0.95)\n",
    "    return net\n",
    "\n",
    "_ = train_agent(episodes=5)\n",
    "print(\"Entrenamiento demo completado.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
