{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512c37a2",
   "metadata": {},
   "source": [
    "# Neuneier (1997) — Q-Learning multi‑activo con aversión al riesgo (utilidad)\n",
    "\n",
    "Baseline educativo: portafolio con **N activos** + cash, política discreta de pesos por *softmax*, utilidad log para aversión al riesgo, costos de rebalanceo.\n",
    "\n",
    "\n",
    "**Contenido:**\n",
    "1) Serie sintética multiactivo.\n",
    "2) Política parametrizada con logits -> pesos via softmax.\n",
    "3) Recompensa = utilidad log(wealth_{t+1}/wealth_t).\n",
    "4) Q-learning sobre estados discretizados de retornos recientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d07f8",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abb5dc",
   "metadata": {},
   "source": [
    "## 2) Datos multiactivo sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c352897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthetic_multi_returns(T=2000, N=4, ar=0.03, sigma=0.012, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    R = np.zeros((T, N))\n",
    "    for i in range(N):\n",
    "        for t in range(1, T):\n",
    "            R[t, i] = ar * R[t-1, i] + rng.normal(0, sigma)\n",
    "    return R\n",
    "\n",
    "R = synthetic_multi_returns()\n",
    "plt.figure()\n",
    "plt.plot(R[:400, :])\n",
    "plt.title('Rendimientos sintéticos multiactivo (primeros 400)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab8e89",
   "metadata": {},
   "source": [
    "## 3) MDP y Q-Learning con utilidad log y costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretize_vec(x, bins):\n",
    "    return np.digitize(x, bins) - 1\n",
    "\n",
    "class QLearnerMulti:\n",
    "    def __init__(self, R, lookback=3, bins=5, gamma=0.99, alpha=0.2,\n",
    "                 cost_rebalance=0.0005, temp=0.5):\n",
    "        # R: T x N returns\n",
    "        self.R = R\n",
    "        self.T, self.N = R.shape\n",
    "        self.lookback = lookback\n",
    "        self.bins = bins\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.cost_rebalance = cost_rebalance\n",
    "        self.temp = temp\n",
    "        # Discretización por cuantiles comunes\n",
    "        flat = R.flatten()\n",
    "        qs = np.quantile(flat, np.linspace(0,1,bins+1))[1:-1]\n",
    "        self.qs = qs\n",
    "        # Acciones: asignaciones discretas por argmax de logits sobre N+1 (incluye cash)\n",
    "        self.A = self.N + 1  # elegir 1 activo o cash en cada t (simple)\n",
    "        self.S = (bins**(self.N*lookback))  # upper bound, no se materializa completa\n",
    "        # Tabla Q como dict: key=(tuple estado, accion) -> valor\n",
    "        self.Q = {}\n",
    "\n",
    "    def state_at(self, t):\n",
    "        # estado: bins de los últimos 'lookback' retornos por activo\n",
    "        vals = []\n",
    "        for L in range(self.lookback):\n",
    "            r = self.R[t-L-1]\n",
    "            for i in range(self.N):\n",
    "                vals.append(discretize_vec(r[i], self.qs))\n",
    "        return tuple(vals)\n",
    "\n",
    "    def boltzmann(self, q_vec):\n",
    "        z = (q_vec / max(1e-8, self.temp))\n",
    "        z = z - np.max(z)\n",
    "        p = np.exp(z)\n",
    "        return p / p.sum()\n",
    "\n",
    "    def step(self, w, a, t):\n",
    "        # a: índice 0..N => N es cash\n",
    "        # construir nuevo vector de pesos one-hot\n",
    "        w_new = np.zeros(self.N+1)\n",
    "        w_new[a] = 1.0\n",
    "        # costo de rebalanceo ~ L1\n",
    "        cost = self.cost_rebalance * np.sum(np.abs(w_new - w))\n",
    "        # retorno de portafolio sobre activos (cash = 0)\n",
    "        r_port = np.dot(w_new[:self.N], self.R[t])\n",
    "        growth = max(1e-8, 1.0 + r_port)\n",
    "        reward = np.log(growth) - cost  # utilidad log\n",
    "        return w_new, reward\n",
    "\n",
    "    def train(self, epochs=3):\n",
    "        # pesos iniciales en cash\n",
    "        for ep in range(epochs):\n",
    "            w = np.zeros(self.N+1); w[-1] = 1.0\n",
    "            for t in range(self.lookback, self.T-1):\n",
    "                s = self.state_at(t)\n",
    "                # construir vector Q para acciones\n",
    "                q_vec = np.array([self.Q.get((s, a), 0.0) for a in range(self.A)])\n",
    "                p = self.boltzmann(q_vec)\n",
    "                a = np.random.choice(self.A, p=p)\n",
    "                w_next, r = self.step(w, a, t)\n",
    "                s_next = self.state_at(t+1)\n",
    "                q_next = np.array([self.Q.get((s_next, ap), 0.0) for ap in range(self.A)])\n",
    "                td_target = r + self.gamma * np.max(q_next)\n",
    "                td_error = td_target - self.Q.get((s, a), 0.0)\n",
    "                self.Q[(s, a)] = self.Q.get((s, a), 0.0) + self.alpha * td_error\n",
    "                w = w_next\n",
    "\n",
    "    def backtest(self):\n",
    "        w = np.zeros(self.N+1); w[-1] = 1.0\n",
    "        wealth = [1.0]\n",
    "        actions = []\n",
    "        for t in range(self.lookback, self.T-1):\n",
    "            s = self.state_at(t)\n",
    "            q_vec = np.array([self.Q.get((s, a), 0.0) for a in range(self.A)])\n",
    "            a = int(np.argmax(q_vec))\n",
    "            actions.append(a)\n",
    "            w, r = self.step(w, a, t)\n",
    "            wealth.append(wealth[-1] * np.exp(r))  # inversa de log-utility\n",
    "        return np.array(wealth), np.array(actions)\n",
    "\n",
    "qlm = QLearnerMulti(R, lookback=3, bins=5, cost_rebalance=0.0005)\n",
    "qlm.train(epochs=5)\n",
    "wealth, acts = qlm.backtest()\n",
    "plt.figure()\n",
    "plt.plot(wealth)\n",
    "plt.title('Wealth (Q-learning multi‑activo con utilidad log)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f931b0",
   "metadata": {},
   "source": [
    "## 4) Notas\n",
    "- Acción simplificada a *selección de un activo*; extender a pesos continuos con política softmax paramétrica.\n",
    "- Cambiar utilidad por Sharpe penalizado si se desea.\n",
    "- Añadir *constraints* realistas y *transaction schedules*. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
