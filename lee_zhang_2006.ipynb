{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86612de2",
   "metadata": {},
   "source": [
    "# Lee & Zhang (2006) — Q-learning incremental para trading\n",
    "MDP de retornos discretizados y posición. Epsilon-greedy. Recompensa = Δwealth - costos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2356be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthetic_returns(n=4000, ar=0.04, sigma=0.012, seed=6):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    r = np.zeros(n)\n",
    "    for t in range(1,n):\n",
    "        r[t] = ar*r[t-1] + rng.normal(0, sigma)\n",
    "    return r\n",
    "\n",
    "r = synthetic_returns()\n",
    "bins = np.quantile(r, [0.2,0.4,0.6,0.8])\n",
    "def disc_ret(x): return int(np.digitize(x, bins))\n",
    "\n",
    "A = [-1,0,1]\n",
    "gamma = 0.99; alpha = 0.2; eps = 0.1; cost = 0.0002\n",
    "\n",
    "# Q[(b, pos, a)] -> value\n",
    "Q = {}\n",
    "\n",
    "def getQ(b, pos, a): return Q.get((b,pos,a), 0.0)\n",
    "def setQ(b, pos, a, val): Q[(b,pos,a)] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step(pos, a, ret):\n",
    "    pnl = a*ret - (0.0002 if a!=pos else 0.0)\n",
    "    pos_next = a\n",
    "    return pos_next, pnl\n",
    "\n",
    "pos = 0; wealth = [1.0]\n",
    "for ep in range(6):\n",
    "    pos = 0\n",
    "    for t in range(1,len(r)-1):\n",
    "        b = disc_ret(r[t-1])\n",
    "        # epsilon-greedy\n",
    "        if rng.random() < eps:\n",
    "            a = rng.choice(A)\n",
    "        else:\n",
    "            qs = [getQ(b,pos,a) for a in A]\n",
    "            a = A[int(np.argmax(qs))]\n",
    "        pos_next, rew = step(pos, a, r[t])\n",
    "        b_next = disc_ret(r[t])\n",
    "        # TD target\n",
    "        q_next = max(getQ(b_next,pos_next,ap) for ap in A)\n",
    "        td = rew + gamma*q_next - getQ(b,pos,a)\n",
    "        setQ(b,pos,a, getQ(b,pos,a) + alpha*td)\n",
    "        pos = pos_next\n",
    "\n",
    "# Backtest greedy\n",
    "pos = 0; wealth = [1.0]\n",
    "for t in range(1,len(r)):\n",
    "    b = disc_ret(r[t-1])\n",
    "    qs = [getQ(b,pos,a) for a in A]\n",
    "    a = A[int(np.argmax(qs))]\n",
    "    pnl = a*r[t] - (0.0002 if a!=pos else 0.0)\n",
    "    wealth.append(wealth[-1]*(1+pnl))\n",
    "    pos = a\n",
    "\n",
    "plt.figure(); plt.plot(wealth); plt.title('Wealth (Q-learning greedy)'); plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
