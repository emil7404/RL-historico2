{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5660d79",
   "metadata": {},
   "source": [
    "# Lee (2002) — Programación Dinámica para trading discreto\n",
    "Value Iteration en un MDP discretizado de retornos y posición. Acciones {-1,0,1}. Recompensa = Δwealth - costos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6ac02",
   "metadata": {},
   "source": [
    "## Entorno y discretización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea16903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthetic_returns(n=2000, ar=0.03, sigma=0.01, seed=2):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    r = np.zeros(n)\n",
    "    for t in range(1,n):\n",
    "        r[t] = ar*r[t-1] + rng.normal(0, sigma)\n",
    "    return r\n",
    "\n",
    "r = synthetic_returns()\n",
    "bins = np.quantile(r, [0.2,0.4,0.6,0.8])\n",
    "\n",
    "def disc_ret(x): return int(np.digitize(x, bins))\n",
    "\n",
    "A = [-1,0,1]  # short, flat, long\n",
    "gamma = 0.98\n",
    "cost = 0.0003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4435ca",
   "metadata": {},
   "source": [
    "## Estados: (bin_ret_{t-1}, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3493227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construir transiciones empíricas P(s'|s,a) a partir de datos\n",
    "def build_transitions(r):\n",
    "    states = []\n",
    "    for t in range(1, len(r)):\n",
    "        states.append((disc_ret(r[t-1]),))  # solo un bin de retorno rezagado\n",
    "    return states\n",
    "\n",
    "states = list(set(build_transitions(r)))\n",
    "pos_space = [-1,0,1]\n",
    "S = [(s[0], p) for s in states for p in pos_space]\n",
    "S_index = {s:i for i,s in enumerate(S)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b99901",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reward(pos, pos_next, ret):\n",
    "    # Δwealth ~ pos_next * ret - costo por cambiar\n",
    "    sw = pos_next*ret - (cost if pos_next!=pos else 0.0)\n",
    "    return sw\n",
    "\n",
    "V = np.zeros(len(S))\n",
    "pi = {s:0 for s in S}  # acción por estado índice de A\n",
    "\n",
    "for it in range(50):\n",
    "    V_new = np.zeros_like(V)\n",
    "    for s in S:\n",
    "        i = S_index[s]\n",
    "        b, pos = s\n",
    "        # enumerar acciones\n",
    "        vals = []\n",
    "        for a in A:\n",
    "            # determinista: acción define pos_next=a\n",
    "            ret_exp = 0.0; cnt = 0\n",
    "            # expectativa empírica condicionada al bin b\n",
    "            for t in range(1, len(r)):\n",
    "                if disc_ret(r[t-1]) == b:\n",
    "                    r_t = r[t]\n",
    "                    b_next = disc_ret(r_t)\n",
    "                    s_next = (b_next, a)\n",
    "                    rew = reward(pos, a, r_t)\n",
    "                    ret_exp += rew + gamma * V[S_index[s_next]]\n",
    "                    cnt += 1\n",
    "            vals.append(ret_exp / max(1,cnt))\n",
    "        best = int(np.argmax(vals))\n",
    "        V_new[i] = vals[best]\n",
    "        pi[s] = A[best]\n",
    "    if np.max(np.abs(V_new - V)) < 1e-6: break\n",
    "    V = V_new\n",
    "\n",
    "print(\"VI done. States:\", len(S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c22028",
   "metadata": {},
   "source": [
    "## Backtest con la política óptima encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos = 0\n",
    "wealth = [1.0]\n",
    "for t in range(1,len(r)):\n",
    "    b = disc_ret(r[t-1])\n",
    "    a = pi[(b, pos)]\n",
    "    pnl = a*r[t] - (cost if a!=pos else 0.0)\n",
    "    wealth.append(wealth[-1] * (1+pnl))\n",
    "    pos = a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(); plt.plot(wealth); plt.title('Wealth (Value Iteration policy)'); plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
