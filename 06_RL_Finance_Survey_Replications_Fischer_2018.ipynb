{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e044c6bb",
   "metadata": {},
   "source": [
    "# Plantilla de replicaciones seg√∫n Fischer (2018): critic-only, actor-only, actor-critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "T=2000\n",
    "price = 100 + np.cumsum(np.random.normal(0,0.2,size=T))\n",
    "ret = np.diff(np.r_[price[0], price])\n",
    "rsi = 50 + 50*np.tanh(np.convolve(ret, np.ones(14)/14, mode=\"same\"))\n",
    "X = np.stack([ret, rsi/100.0], axis=1).astype(np.float32)\n",
    "\n",
    "W = np.zeros((2,3))\n",
    "alpha=0.01; gamma=0.95\n",
    "for t in range(T-1):\n",
    "    q = X[t]@W\n",
    "    a = q.argmax()\n",
    "    r = (price[t+1]-price[t]) * (1 if a==1 else 0)\n",
    "    q_next = X[t+1]@W\n",
    "    td = r + gamma*q_next.max() - q[a]\n",
    "    W[:,a] += alpha*td*X[t]\n",
    "print(\"Critic-only listo (demo).\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(2,32), nn.Tanh(),\n",
    "                               nn.Linear(32,3))\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "pi=Policy(); opt=optim.Adam(pi.parameters(), lr=1e-3)\n",
    "for ep in range(3):\n",
    "    logps=[]; rewards=[]\n",
    "    for t in range(T-1):\n",
    "        logits=pi(torch.tensor(X[t]).unsqueeze(0))\n",
    "        dist=torch.distributions.Categorical(logits=logits)\n",
    "        a=dist.sample()\n",
    "        logps.append(dist.log_prob(a))\n",
    "        r=(price[t+1]-price[t]) * (1 if a.item()==1 else 0)\n",
    "        rewards.append(r)\n",
    "    G=[]; g=0.0\n",
    "    for r in reversed(rewards):\n",
    "        g= r + 0.99*g\n",
    "        G.append(g)\n",
    "    G=torch.tensor(list(reversed(G))).float()\n",
    "    L=-(torch.stack(logps)*((G - G.mean())/(G.std()+1e-8))).sum()\n",
    "    opt.zero_grad(); L.backward(); opt.step()\n",
    "print(\"Actor-only listo (demo).\")\n",
    "\n",
    "class A2C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body=nn.Sequential(nn.Linear(2,64), nn.ReLU())\n",
    "        self.pi=nn.Linear(64,3)\n",
    "        self.v =nn.Linear(64,1)\n",
    "    def forward(self,x):\n",
    "        h=self.body(x)\n",
    "        return self.pi(h), self.v(h)\n",
    "ac=A2C(); opt=optim.Adam(ac.parameters(), lr=1e-3)\n",
    "for ep in range(3):\n",
    "    for t in range(T-1):\n",
    "        logits, v = ac(torch.tensor(X[t]).unsqueeze(0))\n",
    "        dist=torch.distributions.Categorical(logits=logits)\n",
    "        a=dist.sample()\n",
    "        r=(price[t+1]-price[t]) * (1 if a.item()==1 else 0)\n",
    "        with torch.no_grad():\n",
    "            _, v2 = ac(torch.tensor(X[t+1]).unsqueeze(0))\n",
    "        td = r + 0.99*v2.item() - v.item()\n",
    "        pi_loss = -dist.log_prob(a)*td\n",
    "        v_loss = (v - torch.tensor([[r]]))**2\n",
    "        loss = pi_loss + 0.5*v_loss.mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "print(\"Actor-Critic listo (demo).\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
