{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f1267",
   "metadata": {},
   "source": [
    "# Deep Q-Learning en juegos idealizados de series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16971e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import random\n",
    "\n",
    "class UnivariateGame:\n",
    "    def __init__(self, T=4000):\n",
    "        x=np.linspace(0, 50, T)\n",
    "        self.price = 100 + 2*np.sin(x) + np.random.normal(0,0.2,size=T)\n",
    "        self.T=T; self.t=0; self.pos=0.0\n",
    "    def reset(self):\n",
    "        self.t=0; self.pos=0.0\n",
    "        return np.array([self.price[self.t]]).astype(np.float32)\n",
    "    def step(self,a):\n",
    "        done=False; r=0.0\n",
    "        p=self.price[self.t]\n",
    "        if a==1: self.pos=1.0\n",
    "        if a==2: self.pos=0.0\n",
    "        self.t+=1\n",
    "        if self.t>=self.T: done=True\n",
    "        else:\n",
    "            p2=self.price[self.t]\n",
    "            r = self.pos*(p2-p)\n",
    "        return np.array([self.price[self.t-1]]).astype(np.float32), float(r), done, {}\n",
    "\n",
    "class BivariateGame:\n",
    "    def __init__(self, T=4000):\n",
    "        rng=np.random.default_rng(1)\n",
    "        self.price = 100 + np.cumsum(rng.normal(0,0.2,size=T))\n",
    "        fut = np.append(np.diff(self.price, prepend=self.price[0]), 0)\n",
    "        self.signal = fut + rng.normal(0, 0.2, size=T)\n",
    "        self.T=T; self.t=0; self.pos=0.0\n",
    "    def reset(self):\n",
    "        self.t=0; self.pos=0.0\n",
    "        return np.array([self.price[self.t], self.signal[self.t]]).astype(np.float32)\n",
    "    def step(self,a):\n",
    "        done=False; r=0.0\n",
    "        p=self.price[self.t]\n",
    "        if a==1: self.pos=1.0\n",
    "        if a==2: self.pos=0.0\n",
    "        self.t+=1\n",
    "        if self.t>=self.T: done=True\n",
    "        else:\n",
    "            p2=self.price[self.t]\n",
    "            r = self.pos*(p2-p)\n",
    "        return np.array([self.price[self.t-1], self.signal[self.t-1]]).astype(np.float32), float(r), done, {}\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, obs, acts=3):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(obs,64), nn.ReLU(),\n",
    "                               nn.Linear(64,64), nn.ReLU(),\n",
    "                               nn.Linear(64,acts))\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "def train(env, obs_dim, episodes=10):\n",
    "    net=QNet(obs_dim).train()\n",
    "    tgt=QNet(obs_dim).train()\n",
    "    tgt.load_state_dict(net.state_dict())\n",
    "    opt=optim.Adam(net.parameters(), lr=1e-3)\n",
    "    gamma=0.99; eps=1.0\n",
    "    for ep in range(episodes):\n",
    "        s=env.reset(); done=False\n",
    "        while not done:\n",
    "            if random.random()<eps:\n",
    "                a=random.randrange(3)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a=net(torch.tensor(s).unsqueeze(0)).argmax(1).item()\n",
    "            s2,r,done,_=env.step(a)\n",
    "            with torch.no_grad():\n",
    "                y=r + (0 if done else gamma*tgt(torch.tensor(s2).unsqueeze(0)).max(1)[0].item())\n",
    "            q=net(torch.tensor(s).unsqueeze(0))[0,a]\n",
    "            loss=(q - y)**2\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            s=s2\n",
    "        if ep%2==1: tgt.load_state_dict(net.state_dict())\n",
    "        eps=max(0.05, eps*0.9)\n",
    "    return net\n",
    "\n",
    "_ = train(UnivariateGame(), 1, episodes=5)\n",
    "_ = train(BivariateGame(), 2, episodes=5)\n",
    "print(\"DQN demo entrenado en ambos juegos.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
