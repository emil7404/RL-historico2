{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d95684f",
   "metadata": {},
   "source": [
    "# Moody & Wu (1997) — Optimización en línea con razón de Sharpe diferencial\n",
    "\n",
    "Baseline educativo: estrategia parametrizada \\(s_t = \tanh(w^\top x_t)\\) y **entrenamiento en línea** para maximizar una aproximación de la **diferential Sharpe**.\n",
    "\n",
    "\n",
    "Incluye:\n",
    "1) Datos sintéticos con tendencia AR + ruido.\n",
    "2) Señal paramétrica y PnL.\n",
    "3) Estimación recursiva de media/varianza.\n",
    "4) Paso de gradiente aproximado sobre un proxy diferenciable del Sharpe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972bd49",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46882c23",
   "metadata": {},
   "source": [
    "## 2) Serie sintética tipo AR(1) en rendimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e52b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthetic_returns(n=4000, ar=0.05, sigma=0.01, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    r = np.zeros(n, dtype=float)\n",
    "    for t in range(1, n):\n",
    "        r[t] = ar * r[t-1] + rng.normal(0, sigma)\n",
    "    return r\n",
    "\n",
    "r = synthetic_returns()\n",
    "plt.figure()\n",
    "plt.plot(r[:500])\n",
    "plt.title('Rendimientos sintéticos (primeros 500)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a866f",
   "metadata": {},
   "source": [
    "## 3) Señal paramétrica y PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features(r, k=5):\n",
    "    # k rezagos como features\n",
    "    X = []\n",
    "    for t in range(k, len(r)):\n",
    "        X.append(r[t-k:t][::-1])\n",
    "    return np.array(X), r[k:]\n",
    "\n",
    "def pnl_from_signal(signal, ret, cost=0.0002):\n",
    "    # PnL_t = signal_{t-1} * ret_t - cost * |Δsignal_t|\n",
    "    ds = np.diff(signal, prepend=0.0)\n",
    "    pnl = signal * ret - cost * np.abs(ds)\n",
    "    return pnl\n",
    "\n",
    "k = 10\n",
    "X, y = features(r, k=k)\n",
    "# inicialización\n",
    "w = np.zeros(k)\n",
    "signal = np.tanh(X @ w)\n",
    "pnl = pnl_from_signal(signal, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d68fe3",
   "metadata": {},
   "source": [
    "## 4) Estimación recursiva de media/varianza y entrenamiento en línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85438c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def online_train_sharpe(X, y, epochs=1, lr=0.05, beta=0.01, ema=0.01, cost=0.0002):\n",
    "    # beta: pequeño término para estabilizar varianza\n",
    "    # ema: factor para EMA de media y varianza\n",
    "    w = np.zeros(X.shape[1])\n",
    "    m = 0.0  # media EMA\n",
    "    v = 1e-4  # var EMA inicial\n",
    "    history = {\"SR\":[]}\n",
    "    for ep in range(epochs):\n",
    "        for t in range(len(y)):\n",
    "            x = X[t]\n",
    "            s = np.tanh(x @ w)\n",
    "            # forward\n",
    "            if t == 0:\n",
    "                ds = s\n",
    "            else:\n",
    "                s_prev = np.tanh(X[t-1] @ w)\n",
    "                ds = s - s_prev\n",
    "            pnl = s * y[t] - cost * abs(ds)\n",
    "            # actualizar momentos\n",
    "            m = (1-ema)*m + ema*pnl\n",
    "            v = (1-ema)*v + ema*(pnl - m)**2\n",
    "            # proxy diferenciable para Sharpe ~ m / sqrt(v + beta)\n",
    "            sr_proxy = m / np.sqrt(v + beta)\n",
    "            history[\"SR\"].append(sr_proxy)\n",
    "            # gradiente aproximado por REINFORCE-like surrogate\n",
    "            # d pnl / d w ≈ (1 - s**2) * x * y[t]  - cost * sign(ds) * (1 - s**2) * x\n",
    "            ds_sign = np.tanh(1000*ds)  # aprox suave de sign\n",
    "            dpnl_dw = (1 - s**2) * x * y[t] - cost * ds_sign * (1 - s**2) * x\n",
    "            # d sr / d pnl ≈ ∂(m / sqrt(v+β)) / ∂pnl usando aproximación EMA -> tratamos pnl como influir m,v localmente\n",
    "            d_sr_dm = 1/np.sqrt(v + beta)\n",
    "            d_sr_dv = -0.5*m*(v+beta)**(-1.5)\n",
    "            # aproximamos ∂m/∂pnl ≈ ema, ∂v/∂pnl ≈ 2*ema*(pnl - m)\n",
    "            d_sr_dpnl = d_sr_dm*ema + d_sr_dv*(2*ema*(pnl - m))\n",
    "            grad = d_sr_dpnl * dpnl_dw\n",
    "            w += lr * grad\n",
    "    return w, history\n",
    "\n",
    "w_tr, hist = online_train_sharpe(X, y, epochs=3, lr=0.03, ema=0.02, cost=0.0003)\n",
    "signal_tr = np.tanh(X @ w_tr)\n",
    "pnl_tr = pnl_from_signal(signal_tr, y)\n",
    "# SR ex-post simple\n",
    "SR = pnl_tr.mean() / (pnl_tr.std()+1e-9)\n",
    "print(\"Sharpe ex-post:\", SR)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pnl_tr))\n",
    "plt.title('Equity curve (cumulative PnL)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801b791",
   "metadata": {},
   "source": [
    "## 5) Notas\n",
    "- Proxy del gradiente del Sharpe diferencial, enfoque didáctico.\n",
    "- Extender con RTRL/GRU si se desea memoria explícita.\n",
    "- Evaluar robustez con *walk-forward* y *rolling windows*. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
