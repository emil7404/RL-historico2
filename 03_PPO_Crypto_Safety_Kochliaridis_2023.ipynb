{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d41b15",
   "metadata": {},
   "source": [
    "# PPO para Cripto con Mecanismos de Seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "class CryptoEnv:\n",
    "    def __init__(self, T=4000, fee=0.001):\n",
    "        self.T=T; self.fee=fee\n",
    "        rng = np.random.default_rng(0)\n",
    "        self.p = 20000*np.exp(np.cumsum(rng.normal(0, 0.0015, size=T)))\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.t=0; self.pos=0; self.cash=1.0; self.nav=1.0; self.entry=None\n",
    "        self.max_nav=1.0\n",
    "        return self._obs()\n",
    "    def _obs(self):\n",
    "        lo=max(0,self.t-50); x=self.p[lo:self.t+1]\n",
    "        x=(x - x.mean())/(x.std()+1e-6)\n",
    "        x = np.pad(x, (51-len(x),0))\n",
    "        return x.astype(np.float32)\n",
    "    def step(self, a):\n",
    "        done=False\n",
    "        price=self.p[self.t]\n",
    "        reward=0.0\n",
    "        if a==1 and self.pos==0:\n",
    "            size=self.cash*(1-self.fee)\n",
    "            self.pos = size/price\n",
    "            self.cash = 0.0\n",
    "            self.entry = price\n",
    "        elif a==2 and self.pos>0:\n",
    "            self.cash = self.pos*price*(1-self.fee)\n",
    "            self.pos = 0.0\n",
    "            self.entry=None\n",
    "        self.nav = self.cash + self.pos*price\n",
    "        self.max_nav = max(self.max_nav, self.nav)\n",
    "        dd = (self.max_nav - self.nav)/self.max_nav\n",
    "        reward = np.log(self.nav+1e-12) - 0.5*dd\n",
    "        self.t += 1\n",
    "        if self.t >= self.T-1:\n",
    "            if self.pos>0:\n",
    "                self.cash = self.pos*self.p[self.t]*(1-self.fee)\n",
    "                self.pos=0.0\n",
    "            self.nav = self.cash\n",
    "            done=True\n",
    "        return self._obs(), float(reward), done, {\"nav\": self.nav}\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs=51, acts=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(obs,128), nn.Tanh(),\n",
    "                                 nn.Linear(128,128), nn.Tanh())\n",
    "        self.pi = nn.Linear(128, acts)\n",
    "        self.v  = nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        h=self.net(x)\n",
    "        return self.pi(h), self.v(h)\n",
    "\n",
    "def ppo_train(epochs=2, steps=2048, gamma=0.99, lam=0.95, clip=0.2):\n",
    "    env=CryptoEnv()\n",
    "    ac=ActorCritic().train()\n",
    "    opt=torch.optim.Adam(ac.parameters(), lr=3e-4)\n",
    "    for ep in range(epochs):\n",
    "        s=env.reset()\n",
    "        buf=[]\n",
    "        for t in range(steps):\n",
    "            x=torch.tensor(s).unsqueeze(0)\n",
    "            logits, v = ac(x)\n",
    "            prob=torch.distributions.Categorical(logits=logits)\n",
    "            a=prob.sample().item()\n",
    "            s2, r, d, info = env.step(a)\n",
    "            buf.append((s,a,r,prob.log_prob(torch.tensor(a)), v.item(), d))\n",
    "            s=s2\n",
    "            if d: s=env.reset()\n",
    "        _, last_v = ac(torch.tensor(s).unsqueeze(0))\n",
    "        values=np.array([b[4] for b in buf]+[last_v.item()])\n",
    "        rewards=np.array([b[2] for b in buf])\n",
    "        dones=np.array([b[5] for b in buf])\n",
    "        adv=np.zeros_like(rewards); gae=0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + gamma*(1-dones[i])*values[i+1] - values[i]\n",
    "            gae = delta + gamma*lam*(1-dones[i])*gae\n",
    "            adv[i]=gae\n",
    "        ret = adv + values[:-1]\n",
    "        states=torch.tensor(np.stack([b[0] for b in buf])).float()\n",
    "        acts=torch.tensor([b[1] for b in buf]).long()\n",
    "        old_logp=torch.stack([b[3] for b in buf]).detach()\n",
    "        adv_t=torch.tensor(adv).float()\n",
    "        ret_t=torch.tensor(ret).float()\n",
    "        for _ in range(4):\n",
    "            logits, v = ac(states)\n",
    "            dist=torch.distributions.Categorical(logits=logits)\n",
    "            logp=dist.log_prob(acts)\n",
    "            ratio=torch.exp(logp - old_logp)\n",
    "            obj1=ratio*adv_t\n",
    "            obj2=torch.clamp(ratio, 1-clip, 1+clip)*adv_t\n",
    "            pi_loss=-(torch.min(obj1, obj2)).mean()\n",
    "            v_loss=((v.squeeze()-ret_t)**2).mean()\n",
    "            loss=pi_loss + 0.5*v_loss - 0.01*dist.entropy().mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return ac\n",
    "\n",
    "class NConsecutiveFilter:\n",
    "    def __init__(self, N=3):\n",
    "        self.N=N; self.hist=[]\n",
    "    def allow(self, action):\n",
    "        self.hist.append(action)\n",
    "        self.hist=self.hist[-self.N:]\n",
    "        if len(self.hist)==self.N and len(set(self.hist))==1 and self.hist[-1]!=0:\n",
    "            return 0\n",
    "        return action\n",
    "\n",
    "class SmurfingGuard:\n",
    "    def __init__(self, threshold=-0.002):\n",
    "        self.threshold=threshold\n",
    "    def allow(self, action, recent_returns):\n",
    "        mr = np.mean(recent_returns[-20:]) if len(recent_returns)>=20 else 0.0\n",
    "        if mr < self.threshold:\n",
    "            return 0\n",
    "        return action\n",
    "\n",
    "_ = ppo_train(epochs=2)\n",
    "print(\"Demo PPO + filtros lista para extender.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
