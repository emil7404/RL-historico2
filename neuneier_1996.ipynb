{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151d369f",
   "metadata": {},
   "source": [
    "# Neuneier (1996) — Q-Learning para asignación de activos (2 monedas)\n",
    "\n",
    "Baseline educativo inspirado en Neuneier (1996): Q-Learning para decidir si mantener **DM** o **USD** bajo costos de transacción.\n",
    "\n",
    "\n",
    "**Contenido:**\n",
    "1) Generación de una serie artificial de tipo “sube con probabilidad, cae con choques”.\n",
    "2) Definición del MDP con estado = (bin de tipo de cambio, signo de posición).\n",
    "3) Q-Learning con exploración Boltzmann.\n",
    "4) Backtest y gráfica de wealth.\n",
    "\n",
    "**Advertencia:** simplificado para docencia, no reproduce todos los detalles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95100cf9",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6477a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f295f6",
   "metadata": {},
   "source": [
    "## 2) Serie artificial de tipo de cambio USD/DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def artificial_fx(n=3000, low=1.0, high=2.0, trend=0.002, crash_prob_base=0.001, crash_scale=0.15, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.empty(n, dtype=float)\n",
    "    x[0] = (low + high)/2\n",
    "    for t in range(1, n):\n",
    "        level = x[t-1]\n",
    "        # drift up\n",
    "        mu = trend\n",
    "        # higher level => higher crash prob\n",
    "        crash_prob = crash_prob_base + 0.02 * max(0.0, (level - (low+high)/2) / (high - low))\n",
    "        eps = rng.normal(0, 0.003)\n",
    "        x[t] = np.clip(level * (1 + mu + eps), low, high)\n",
    "        if rng.random() < crash_prob:\n",
    "            x[t] = max(low, x[t] * (1 - crash_scale * rng.uniform(0.5, 1.0)))\n",
    "    return x\n",
    "\n",
    "fx = artificial_fx()\n",
    "plt.figure()\n",
    "plt.plot(fx)\n",
    "plt.title('Artificial FX (USD in DM)')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc0687",
   "metadata": {},
   "source": [
    "## 3) MDP y Q-Learning\n",
    "Estado = (bin_fx, pos) con pos∈{0: DM, 1: USD}. Acciones = {0: mantener, 1: cambiar}. Recompensa = Δwealth neta de costos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretize(x, bins):\n",
    "    return np.digitize(x, bins) - 1\n",
    "\n",
    "class QLearnerFX:\n",
    "    def __init__(self, fx, n_bins=10, cost_fixed=0.0005, cost_prop=0.0005,\n",
    "                 tau=0.5, gamma=0.99, alpha=0.2):\n",
    "        self.fx = fx\n",
    "        self.n = len(fx)\n",
    "        self.bins = np.linspace(fx.min(), fx.max(), n_bins+1)[1:-1]\n",
    "        self.n_bins = n_bins\n",
    "        self.cost_fixed = cost_fixed\n",
    "        self.cost_prop = cost_prop\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        # Q[bin_fx, pos, action]\n",
    "        self.Q = np.zeros((n_bins, 2, 2), dtype=float)\n",
    "\n",
    "    def boltzmann(self, q):\n",
    "        z = (q / max(1e-8, self.tau))\n",
    "        z = z - z.max()\n",
    "        p = np.exp(z)\n",
    "        p = p / p.sum()\n",
    "        return p\n",
    "\n",
    "    def step_reward(self, pos, action, w, t):\n",
    "        # pos: 0 DM, 1 USD\n",
    "        # action: 0 stay, 1 switch\n",
    "        # wealth update over [t, t+1)\n",
    "        rate_t, rate_tp1 = self.fx[t], self.fx[t+1]\n",
    "        new_pos = pos\n",
    "        cost = 0.0\n",
    "        if action == 1:  # switch\n",
    "            cost = self.cost_fixed * w + self.cost_prop * w\n",
    "            new_pos = 1 - pos\n",
    "\n",
    "        # Δwealth\n",
    "        if new_pos == 0:\n",
    "            # holding DM: wealth unchanged except costs\n",
    "            w_tp1 = w - cost\n",
    "        else:\n",
    "            # holding USD: wealth DM changes by rate ratio\n",
    "            # approximate gain by holding USD vs DM\n",
    "            growth = rate_tp1 / rate_t\n",
    "            w_tp1 = w * growth - cost\n",
    "\n",
    "        reward = w_tp1 - w\n",
    "        return new_pos, w_tp1, reward\n",
    "\n",
    "    def train(self, epochs=5, w0=1.0):\n",
    "        for ep in range(epochs):\n",
    "            w = w0\n",
    "            pos = 0  # start in DM\n",
    "            for t in range(self.n-1):\n",
    "                s_bin = discretize(self.fx[t], self.bins)\n",
    "                q = self.Q[s_bin, pos]\n",
    "                p = self.boltzmann(q)\n",
    "                a = np.random.choice([0,1], p=p)\n",
    "                new_pos, w_tp1, r = self.step_reward(pos, a, w, t)\n",
    "                # next state\n",
    "                s_bin_next = discretize(self.fx[t+1], self.bins)\n",
    "                td_target = r + self.gamma * np.max(self.Q[s_bin_next, new_pos])\n",
    "                td_error = td_target - self.Q[s_bin, pos, a]\n",
    "                self.Q[s_bin, pos, a] += self.alpha * td_error\n",
    "                w = w_tp1\n",
    "                pos = new_pos\n",
    "\n",
    "    def policy(self):\n",
    "        # greedy\n",
    "        return np.argmax(self.Q, axis=-1)\n",
    "\n",
    "    def backtest(self, w0=1.0):\n",
    "        w = w0\n",
    "        pos = 0\n",
    "        wealth = [w]\n",
    "        actions = []\n",
    "        for t in range(self.n-1):\n",
    "            s_bin = discretize(self.fx[t], self.bins)\n",
    "            a = np.argmax(self.Q[s_bin, pos])\n",
    "            actions.append(a)\n",
    "            pos, w, r = self.step_reward(pos, a, w, t)\n",
    "            wealth.append(w)\n",
    "        return np.array(wealth), np.array(actions)\n",
    "\n",
    "ql = QLearnerFX(fx)\n",
    "ql.train(epochs=6)\n",
    "wealth, acts = ql.backtest()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wealth)\n",
    "plt.title('Wealth (QL policy)')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('wealth (DM)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d388c",
   "metadata": {},
   "source": [
    "## 4) Notas y próximos pasos\n",
    "- Añadir capital continuo y discretización por cuantiles.\n",
    "- Incluir *risk aversion* vía utilidad log.\n",
    "- Validar contra DP en entorno artificial pequeño.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
